{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6534291f",
   "metadata": {},
   "source": [
    "# WeRateDogs Tweets Data Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee15af8",
   "metadata": {},
   "source": [
    "## Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342afd",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The dataset is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a2c791",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "The datasets for this analysis are not all in one place, they were gathered as described below;\n",
    "- The first part of the data called *twitter-archive-enhanced.csv* has been given to Udacity and was made available to us.\n",
    "- *Image-prediction.csv* file which is the second part was stored in Udacity server and was downloaded programmatically through *https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv* using *requests* command. The file contains three types of predictions of dog's breed from the images uploaded by the users. \n",
    "- For the last part, I querried twitter API using the *tweet_id* present in the *twitter-archive-enhanced.csv* file, and each tweet's JSON data was stored in *tweet_json.txt*. The text file was read line by line to extract needed informations into pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c2fd0",
   "metadata": {},
   "source": [
    "### Data Assessment\n",
    "Having gathered the three datasets, I assessed them visually and programtically using pything packages like info(), describe(), isnull() and others. The following issues is documented\n",
    "#### Quality Issues\n",
    "- *in_reply_to_status_id*, *retweeted_status_timestamp*, *retweeted_status_user_id* *in_reply_to_user_id* columns has null values\n",
    "- *timestamp* not in correct datatype\n",
    "- *name* is not accurate in all cases. some names are picked from the third word of the tweet \n",
    "- some tweets do not contain the dog's name\n",
    "- *rating_numerator* is not accurate in all cases\n",
    "- some values in *rating_numerator* and *rating_denominator* are outrageous\n",
    "- some outrageous values in *rating_numerator and rating_denominator* are actually values after decimal point\n",
    "- *rating_numerator* should be float datatype\n",
    "- some values in *rating_numerator* and *rating_denominator* are meant for more than one dog\n",
    "- missing values in *name*, *doggo*, *floofer*, *pupper* and *puppo* are represented by none.\n",
    "- Dog breed could not be established with the predictions\n",
    "- *tweet_id* is of int datatype instead of string\n",
    "- some tweets are retweets\n",
    "\n",
    "\n",
    "#### Tidiness issues\n",
    "- *doggo*, *floofer*, *pupper* and *puppo* are stages/sizes of dogs and should be in one column\n",
    "- The three datasets should be in one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699f43a",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The first step taken here was to duplicate all the three datasets. This is a standard procedure.\n",
    "The cleaning part is divided into three steps; define, code and test. Each of the documented issues is made to pass through the stages: define how the issue is to be rectified, write the code and then test to make sure it worked fine. Some rows with missing values were dropped and columns not useful for my analysis were dropped too. Lastly, the three datasets was merged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e75e1",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "After proper cleaning, the merged dataset is now ready for further exploration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
